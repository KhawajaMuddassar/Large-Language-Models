base_config:
  average_embeddings: false
  batch_size: 8
  context_length: '256'
  drop_rate: 0.0
  eval_freq: 50
  eval_iter: 20  
  max_steps: None   
  num_workers: 0
  qkv_bias: true
  vocab_size: 50257
  weight_decay: 0.1  
training_config:
  trainable_layers:
    selected: last_block
    options:
      - first
      - last    
  model_size:
    selected: gpt2-small
    options:
      - gpt2-small
      - gpt2-medium
      - gpt2-large
      - gpt2-xl
  weights:
    selected: pretrained
    options:
      - pretrained
      - randon
  trainable_token_pos:
    selected: last
    options:
      - first
      - last
  average_embeddings:
    selected: 'false'
    options:
      - 'false'
      - 'true'
  learning_rate:
    selected: 5e-1
    options:
      - 5e-5
      - 1e-3
      - 1e-4
  context_length:
    selected: '256'
    options:
      - longest_training_example
      - model_context_length
  num_epochs:
    selected: 1
    options:
      - 5
      - 10
      - 50
model_config:
  gpt2-large:
    emb_dim: 1280
    n_heads: 20
    n_layers: 36
  gpt2-medium:
    emb_dim: 1024
    n_heads: 16
    n_layers: 24
  gpt2-small:
    emb_dim: 768
    n_heads: 12
    n_layers: 12
  gpt2-xl:
    emb_dim: 1600
    n_heads: 25
    n_layers: 48